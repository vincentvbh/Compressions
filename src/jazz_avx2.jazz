
param int KYBER_Q = 3329;
param int KYBER_N = 256;

u16 compress4_b0 = 630;
u16 compress4_b1 = 1 << 14;
u16 compress4_mask4 = 0xf;
u16 compress4_shift = 0x1001;
u32[8] compress4_shuffle = {0, 4, 1, 5, 2, 6, 3, 7};

export
fn poly_compress4_avx2_jazz(reg u64 r, reg u64 a){

    reg u256 a0 a1 a2 a3;
    reg u256 b0 b1;
    reg u256 mask4 shift shuffle;
    reg u64 i;

    b0 = #VPBROADCAST_16u16(compress4_b0);
    b1 = #VPBROADCAST_16u16(compress4_b1);
    mask4 = #VPBROADCAST_16u16(compress4_mask4);
    shift = #VPBROADCAST_16u16(compress4_shift);
    shuffle = compress4_shuffle[u256 0];

    i = 0;

    while(i < KYBER_N / 64){
        a0 = (u256)[a + 0 * 32];
        a1 = (u256)[a + 1 * 32];
        a2 = (u256)[a + 2 * 32];
        a3 = (u256)[a + 3 * 32];

        a += 128;

        a0 = #VPMULH_16u16(a0, b0);
        a1 = #VPMULH_16u16(a1, b0);
        a2 = #VPMULH_16u16(a2, b0);
        a3 = #VPMULH_16u16(a3, b0);

        a0 = #VPMULHRS_16u16(a0, b1);
        a1 = #VPMULHRS_16u16(a1, b1);
        a2 = #VPMULHRS_16u16(a2, b1);
        a3 = #VPMULHRS_16u16(a3, b1);

        a0 = #VPAND_256(a0, mask4);
        a1 = #VPAND_256(a1, mask4);
        a2 = #VPAND_256(a2, mask4);
        a3 = #VPAND_256(a3, mask4);

        a0 = #VPACKUS_16u16(a0, a1);
        a2 = #VPACKUS_16u16(a2, a3);

        a0 = #VPMADDUBSW_256(a0, shift);
        a2 = #VPMADDUBSW_256(a2, shift);

        a0 = #VPACKUS_16u16(a0, a2);

        a0 = #VPERMD(shuffle, a0);

        (u256)[r] = a0;
        r += 32;

        i += 1;

    }

}

u16 compress10_b0 = -20553;
u16 compress10_b1 = 20;
u16 compress10_b2 = 1 << 9;
u16 compress10_mask10 = 0x3ff;
u32 compress10_shift = 0x04000001;
u64 compress10_sllv_indx = 12;
u8[32] compress10_shuffle = { 0,  1,  2,  3,  4,  8,  9, 10,
                             11, 12, -1, -1, -1, -1, -1, -1,
                              9, 10, 11, 12, -1, -1, -1, -1,
                             -1, -1,  0,  1,  2,  3,  4,  8};

export
fn poly_compress10_avx2_jazz(reg u64 r, reg u64 a){

    reg u256 a0;
    reg u256 p0 p1;
    reg u128 lo hi;
    reg u256 b0 b1 b2;
    reg u256 mask10 shift sllv_indx shuffle;
    reg u64 i;

    b0 = #VPBROADCAST_16u16(compress10_b0);
    b1 = #VPBROADCAST_16u16(compress10_b1);
    b2 = #VPBROADCAST_16u16(compress10_b2);
    mask10 = #VPBROADCAST_16u16(compress10_mask10);
    shift = #VPBROADCAST_8u32(compress10_shift);
    sllv_indx = #VPBROADCAST_4u64(compress10_sllv_indx);
    shuffle = compress10_shuffle[u256 0];

    i = 0;

    while(i < KYBER_N / 16){

        a0 = (u256)[a];
        a += 32;

        p0 = #VPMULH_16u16(a0, b0);
        p1 = #VPMULL_16u16(a0, b1);
        p0 = #VPADD_16u16(p0, p1);
        p0 = #VPMULHRS_16u16(p0, b2);

        a0 = #VPAND_256(p0, mask10);

        a0 = #VPMADDWD_256(a0, shift);

        a0 = #VPSLLV_8u32(a0, sllv_indx);
        a0 = #VPSRL_4u64(a0, 12);

        a0 = #VPSHUFB_256(a0, shuffle);

        lo = (128u)a0;
        hi = #VEXTRACTI128(a0, 1);

        lo = #VPBLEND_8u16(lo, hi, 0xe0);

        (u128)[r + 0] = lo;
        (u32)[r + 16] = #VPEXTR_32(hi, 0);
        r += 20;

        i += 1;
    }

}


