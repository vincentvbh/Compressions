
param int KYBER_Q = 3329;
param int KYBER_N = 256;

param int KYBER_K = 3;

param int KYBER_VECN = KYBER_K * KYBER_N;

u16 compress4_b0 = 630;
u16 compress4_b1 = 1 << 14;
u16 compress4_mask4 = 0xf;
u16 compress4_shift = 0x1001;
u32[8] compress4_shuffle = {0, 4, 1, 5, 2, 6, 3, 7};

export
fn poly_compress4_avx2_jazz(reg u64 r, reg u64 a){

    reg u256 a0 a1 a2 a3;
    reg u256 b0 b1;
    reg u256 mask4 shift shuffle;
    reg u64 i;

    b0 = #VPBROADCAST_16u16(compress4_b0);
    b1 = #VPBROADCAST_16u16(compress4_b1);
    mask4 = #VPBROADCAST_16u16(compress4_mask4);
    shift = #VPBROADCAST_16u16(compress4_shift);
    shuffle = compress4_shuffle[u256 0];

    i = 0;

    while(i < KYBER_N / 64){
        a0 = (u256)[a + 0 * 32];
        a1 = (u256)[a + 1 * 32];
        a2 = (u256)[a + 2 * 32];
        a3 = (u256)[a + 3 * 32];

        a += 128;

        a0 = #VPMULH_16u16(a0, b0);
        a1 = #VPMULH_16u16(a1, b0);
        a2 = #VPMULH_16u16(a2, b0);
        a3 = #VPMULH_16u16(a3, b0);

        a0 = #VPMULHRS_16u16(a0, b1);
        a1 = #VPMULHRS_16u16(a1, b1);
        a2 = #VPMULHRS_16u16(a2, b1);
        a3 = #VPMULHRS_16u16(a3, b1);

        a0 = #VPAND_256(a0, mask4);
        a1 = #VPAND_256(a1, mask4);
        a2 = #VPAND_256(a2, mask4);
        a3 = #VPAND_256(a3, mask4);

        a0 = #VPACKUS_16u16(a0, a1);
        a2 = #VPACKUS_16u16(a2, a3);

        a0 = #VPMADDUBSW_256(a0, shift);
        a2 = #VPMADDUBSW_256(a2, shift);

        a0 = #VPACKUS_16u16(a0, a2);

        a0 = #VPERMD(shuffle, a0);

        (u256)[r] = a0;
        r += 32;

        i += 1;

    }

}

u16 compress10_b0 = -20553;
u16 compress10_b1 = 20;
u16 compress10_b2 = 1 << 9;
u16 compress10_mask10 = 0x3ff;
u32 compress10_shift = 0x04000001;
u64 compress10_sllv_indx = 12;
u8[32] compress10_shuffle = { 0,  1,  2,  3,  4,  8,  9, 10,
                             11, 12, -1, -1, -1, -1, -1, -1,
                              9, 10, 11, 12, -1, -1, -1, -1,
                             -1, -1,  0,  1,  2,  3,  4,  8};

inline
fn polyvec_compress10_avx2_inline(reg u64 r, stack u16[KYBER_VECN] a){

    reg u256 a0;
    reg u256 p0 p1;
    reg u128 lo hi;
    reg u256 b0 b1 b2;
    reg u256 mask10 shift sllv_indx shuffle;
    inline int i;

    b0 = #VPBROADCAST_16u16(compress10_b0);
    b1 = #VPBROADCAST_16u16(compress10_b1);
    b2 = #VPBROADCAST_16u16(compress10_b2);
    mask10 = #VPBROADCAST_16u16(compress10_mask10);
    shift = #VPBROADCAST_8u32(compress10_shift);
    sllv_indx = #VPBROADCAST_4u64(compress10_sllv_indx);
    shuffle = compress10_shuffle[u256 0];

    for i = 0 to KYBER_VECN / 16 {

        a0 = a[u256 i];

        p0 = #VPMULH_16u16(a0, b0);
        p1 = #VPMULL_16u16(a0, b1);
        p0 = #VPADD_16u16(p0, p1);
        p0 = #VPMULHRS_16u16(p0, b2);

        a0 = #VPAND_256(p0, mask10);

        a0 = #VPMADDWD_256(a0, shift);

        a0 = #VPSLLV_8u32(a0, sllv_indx);
        a0 = #VPSRL_4u64(a0, 12);

        a0 = #VPSHUFB_256(a0, shuffle);

        lo = (128u)a0;
        hi = #VEXTRACTI128(a0, 1);

        lo = #VPBLEND_8u16(lo, hi, 0xe0);

        (u128)[r + 0] = lo;
        (u32)[r + 16] = #VPEXTR_32(hi, 0);
        r += 20;

    }

}

export
fn polyvec_compress10_avx2_jazz(reg u64 r, reg u64 a){

    stack u16[KYBER_VECN] a_stack;
    inline int i;

    for i = 0 to KYBER_VECN {
        a_stack[i] = (u16)[a + 2 * i];
    }

    polyvec_compress10_avx2_inline(r, a_stack);

}

u16 Barrett_b0 = 20159;
u16 Barrett_b1 = 32;
u16 q = KYBER_Q;

inline
fn round_reduce_avx2(reg u256 a qx16 b0 b1) -> reg u256 {
    reg u256 t;
    t = #VPMULH_16u16(a, b0);
    t = #VPMULHRS_16u16(t, b1);
    t = #VPMULL_16u16(t, qx16);
    a = #VPSUB_16u16(a, t);
    return a;
}

inline
fn poly_round_reduce_avx2_inline(reg ptr u16[KYBER_N] ap) -> reg ptr u16[KYBER_N] {

    reg u256 a;
    reg u256 qx16 b0 b1;
    inline int i;

    qx16 = #VPBROADCAST_16u16(q);
    b0 = #VPBROADCAST_16u16(Barrett_b0);
    b1 = #VPBROADCAST_16u16(Barrett_b1);

    for i = 0 to 16 {
        a = ap.[u256 32 * i];
        a = round_reduce_avx2(a, qx16, b0, b1);
        ap.[u256 32 * i] = a;
    }
    return ap;

}

export
fn poly_round_reduce_avx2(reg u64 a){

    stack u16[KYBER_N] a_stack;
    reg ptr u16[KYBER_N] a_ptr;
    inline int i;

    for i = 0 to KYBER_N {
        a_stack[i] = (u16)[a + 2 * i];
    }

    a_ptr = a_stack;
    a_ptr = poly_round_reduce_avx2_inline(a_ptr);
    a_stack = a_ptr;

    for i = 0 to KYBER_N {
        (u16)[a + 2 * i] = a_stack[i];
    }

}
