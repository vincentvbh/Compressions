
export
fn __jazz_poly_compress1(reg u32 des, reg u32 src){

    reg u32[8] a;
    reg u32 b;

    inline int i j;

    // 2580335 = round(2^33 / 3329)
    b = #MOV(2580335 % 65536);
    b = #MOVT(b, 2580335 / 65536);

    for i = 0 to 32 {

        for j = 0 to 8 {
            a[j] = (32s)(u16)[src + j * 2];
        }

        src += 8 * 2;

        for j = 0 to 8 {
            a[j] = #SMMULR(a[j], b);
            a[j] = #UBFX(a[j], 0, 1);
        }

        for j = 1 to 8 {
            a[0] = a[0] | (a[j] << j);
        }

        (u8)[des] = (8u)a[0];
        des += 1;

    }

}

/*
.align 4
.global __asm_poly_compress1
.type __asm_poly_compress1, %function
__asm_poly_compress1:
    push.w {r4-r12, lr}

    movw r2, #:lower16:2580335
    movt r2, #:upper16:2580335
    mov.w r3, #1
    orr.w r3, r3, r3, lsl #16

.rept 32

    ldrsh.w r5, [r1, #2]
    ldrsh.w r6, [r1, #4]
    ldrsh.w r7, [r1, #6]
    ldrsh.w r8, [r1, #8]
    ldrsh.w r9, [r1, #10]
    ldrsh.w r10, [r1, #12]
    ldrsh.w r11, [r1, #14]
    ldrsh.w r4, [r1], #16

    smmulr r4, r4, r2
    smmulr r5, r5, r2
    smmulr r6, r6, r2
    smmulr r7, r7, r2
    smmulr r8, r8, r2
    smmulr r9, r9, r2
    smmulr r10, r10, r2
    smmulr r11, r11, r2

    ubfx.w r4, r4, #0, #1
    ubfx.w r5, r5, #0, #1
    ubfx.w r6, r6, #0, #1
    ubfx.w r7, r7, #0, #1
    ubfx.w r8, r8, #0, #1
    ubfx.w r9, r9, #0, #1
    ubfx.w r10, r10, #0, #1
    ubfx.w r11, r11, #0, #1

    orr.w r4, r4, r5, lsl #1
    orr.w r4, r4, r6, lsl #2
    orr.w r4, r4, r7, lsl #3
    orr.w r4, r4, r8, lsl #4
    orr.w r4, r4, r9, lsl #5
    orr.w r4, r4, r10, lsl #6
    orr.w r4, r4, r11, lsl #7

    strb.w r4, [r0], #1

.endr

    pop.w {r4-r12, pc}
*/
